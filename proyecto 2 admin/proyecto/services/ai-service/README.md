# ü§ñ Servicio de ChatBot IA - Asistente Virtual Inteligente (100% GRATUITO)

## Descripci√≥n

Servicio de chatbot inteligente basado en **Ollama** (LLM local) que proporciona soporte y asistencia en tiempo real a los usuarios del sistema de reservas de licencias de conducir. El chatbot reduce la carga de atenci√≥n presencial y telef√≥nica respondiendo consultas sobre:

- Requisitos y documentaci√≥n para diferentes tipos de licencias
- Proceso de reserva y navegaci√≥n del sistema
- Horarios de atenci√≥n y pol√≠ticas
- Resoluci√≥n de problemas t√©cnicos b√°sicos
- Preguntas frecuentes

> **‚úÖ IMPORTANTE**: Este servicio es **100% GRATUITO** - No requiere API keys externas ni costos por uso. Utiliza modelos de IA ejecut√°ndose localmente en Docker con Ollama.

## Caracter√≠sticas Principales

### üéØ Funcionalidades
- **Conversaciones Contextuales**: Mantiene el contexto de la conversaci√≥n usando sesiones persistentes
- **Knowledge Base Integrada**: Base de conocimientos sobre tr√°mites, licencias y procedimientos
- **Respuestas Inteligentes**: Utiliza modelos LLM locales (Llama 2, Mistral) para respuestas naturales y precisas
- **Detecci√≥n de Contexto**: Detecta en qu√© secci√≥n del sistema est√° el usuario para respuestas m√°s relevantes
- **Historial Persistente**: Guarda el historial de conversaciones en base de datos
- **M√©tricas de Uso**: Recopila estad√≠sticas de uso y rendimiento
- **üÜì Sin Costos**: Corre completamente en tu infraestructura sin costos de API externa

### üèóÔ∏è Arquitectura
- **Alta Disponibilidad**: 2 instancias del servicio con balanceo de carga
- **Base de Datos Replicada**: PostgreSQL con replicaci√≥n primario-r√©plica
- **Cach√© con Redis**: Sesiones y respuestas cacheadas para mejor rendimiento
- **API RESTful**: Endpoints bien documentados con FastAPI
- **Ollama Local**: Servidor de IA ejecut√°ndose en contenedor Docker

## Endpoints de la API

### `POST /chat`
Enviar un mensaje al chatbot y recibir respuesta

**Request:**
```json
{
  "message": "¬øQu√© documentos necesito para licencia clase B?",
  "session_id": "uuid-opcional",
  "context": {
    "current_page": "/reservas",
    "section": "reservations"
  }
}
```

**Response:**
```json
{
  "response": "Para obtener la licencia clase B necesitas...",
  "session_id": "123e4567-e89b-12d3-a456-426614174000",
  "tokens_used": 245,
  "response_time_ms": 1234
}
```

### `GET /chat/history/{session_id}`
Obtener historial completo de una sesi√≥n

### `DELETE /chat/session/{session_id}`
Cerrar/limpiar una sesi√≥n de chat

### `GET /chat/metrics`
Obtener m√©tricas de uso del usuario actual

### `GET /chat/sessions`
Listar todas las sesiones activas del usuario

### `GET /health`
Healthcheck del servicio

## Configuraci√≥n

### Variables de Entorno Requeridas

```env
# ============================================================
# Ollama Configuration (100% GRATUITO - Sin API keys ni costos)
# ============================================================
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama2  # Opciones: llama2, mistral, codellama
OLLAMA_TIMEOUT=60

# Base de Datos
CHATBOT_DB_USER=admin
CHATBOT_DB_PASSWORD=admin
CHATBOT_DB_NAME=chatbot_db
CHATBOT_DB_PORT=5435

# Redis
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=1

# JWT (debe coincidir con auth-service)
SECRET_KEY=un-secreto-muy-fuerte-y-largo
ALGORITHM=HS256

# Servicio
CHATBOT_SERVICE_PORT=8005
```

## Instalaci√≥n y Uso

### Con Docker Compose (Recomendado)

El servicio se despliega autom√°ticamente con el stack completo:

```bash
docker-compose up -d --build
```

**Nota importante**: En el primer inicio, Ollama descargar√° autom√°ticamente el modelo Llama 2 (~4GB). Esto puede tardar varios minutos dependiendo de tu conexi√≥n a internet.

### Verificar que Ollama est√° funcionando

```bash
# Verificar modelos instalados
docker exec ollama_service ollama list

# Descargar modelo adicional (opcional)
docker exec ollama_service ollama pull mistral
```

### Modelos Disponibles

Puedes usar diferentes modelos seg√∫n tus necesidades:
- **llama2** (recomendado): Modelo general de ~4GB
- **mistral**: Modelo m√°s ligero y r√°pido
- **codellama**: Especializado en c√≥digo

Para cambiar el modelo, actualiza la variable `OLLAMA_MODEL` en tu `.env`

### Desarrollo Local

```bash
cd services/ai-service

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno
cp .env.example .env
# Nota: No necesitas configurar API keys - es 100% local

# Aseg√∫rate de que Ollama est√© corriendo
docker-compose up -d ollama

# Ejecutar servicio
python main.py
```

El servicio estar√° disponible en `http://localhost:8005`

## Base de Datos

### Esquema

- **users**: Referencia a usuarios del sistema
- **chat_sessions**: Sesiones de conversaci√≥n
- **chat_messages**: Mensajes individuales (usuario y asistente)
- **chat_metrics**: M√©tricas de uso y rendimiento

### Vistas
- **user_chat_stats**: Estad√≠sticas agregadas por usuario
- **daily_chatbot_metrics**: M√©tricas diarias del sistema

## Knowledge Base

El chatbot cuenta con una base de conocimientos estructurada que incluye:

### Licencias
- Clase B (particular)
- Clase A (profesional)
- Renovaci√≥n
- Duplicado

### Informaci√≥n por Licencia
- Requisitos y documentos necesarios
- Costos y duraci√≥n del tr√°mite
- Restricciones de edad
- Documentaci√≥n digital aceptada

### Procedimientos
- Proceso de reserva paso a paso
- Navegaci√≥n del sistema
- Pol√≠ticas de cancelaci√≥n y reprogramaci√≥n
- Horarios de atenci√≥n

### Soporte T√©cnico
- Problemas comunes y soluciones
- Informaci√≥n de contacto
- Formatos de archivos aceptados

## Integraci√≥n con Frontend

El ChatBot est√° integrado como un widget flotante disponible en todas las p√°ginas del sistema (excepto login/register).

### Caracter√≠sticas del Widget
- **Posici√≥n Flotante**: Bot√≥n en esquina inferior derecha
- **Interfaz Moderna**: Dise√±o limpio y responsivo
- **Persistencia**: Historial guardado en localStorage
- **Contexto Autom√°tico**: Detecta la p√°gina actual del usuario
- **Sugerencias R√°pidas**: Preguntas frecuentes predefinidas
- **Indicadores Visuales**: Loading states y mensajes de error

### Uso desde el C√≥digo

```jsx
import ChatBotWidget from './components/ChatBotWidget';

function App() {
  return (
    <div>
      {/* Tu aplicaci√≥n */}
      <ChatBotWidget />
    </div>
  );
}
```

## Monitoreo

### M√©tricas Disponibles
- N√∫mero de conversaciones por usuario
- Mensajes totales procesados
- Tokens consumidos (costo)
- Tiempo promedio de respuesta
- Tasa de errores

### Integraci√≥n con Prometheus

El servicio expone m√©tricas en formato compatible con Prometheus:

```yaml
# En prometheus.yml
scrape_configs:
  - job_name: 'chatbot-service'
    static_configs:
      - targets: ['chatbot-service-1:8005', 'chatbot-service-2:8005']
```

## üÜì Costos (100% GRATUITO)

### Sin Costos de API Externa
- **‚úÖ Modelo**: Llama 2 / Mistral (ejecut√°ndose localmente)
- **‚úÖ Costo por consulta**: $0.00 USD (completamente gratis)
- **‚úÖ Sin l√≠mites de tokens**: No hay cargos por uso
- **‚úÖ Sin facturaci√≥n externa**: Todo corre en tu infraestructura

### √önicos Requisitos
- **Hardware**: ~4GB de RAM para el modelo (ya incluido en docker-compose.yml)
- **Disco**: ~4-7GB para almacenar el modelo Llama 2
- **CPU**: Al menos 2 cores recomendado (configurable en docker-compose.yml)

### Comparaci√≥n con OpenAI
| Aspecto | Ollama (Esta soluci√≥n) | OpenAI GPT |
|---------|----------------------|------------|
| Costo por consulta | **$0.00** | $0.0005 - $0.001 |
| Requiere API Key | **No** | S√≠ |
| Requiere tarjeta de cr√©dito | **No** | S√≠ |
| L√≠mites de rate | **Ninguno** | S√≠ (depende del plan) |
| Privacidad de datos | **100% local** | Enviado a OpenAI |
| Dependencia de internet | **Solo descarga inicial del modelo** | Siempre requiere conexi√≥n |

## Troubleshooting

### El chatbot no responde
1. Verificar que Ollama est√° corriendo: `docker ps | grep ollama`
2. Revisar logs: `docker logs chatbot_service_1`
3. Verificar que el modelo est√° descargado: `docker exec ollama_service ollama list`
4. Si el modelo no est√°, descargarlo: `docker exec ollama_service ollama pull llama2`

### Error de conexi√≥n con Ollama
- Verificar que el contenedor ollama est√° saludable: `docker ps`
- Comprobar que OLLAMA_BASE_URL apunta a `http://ollama:11434`
- Revisar logs de Ollama: `docker logs ollama_service`

### El modelo se descarga muy lento
- La descarga inicial de ~4GB puede tardar dependiendo de tu conexi√≥n
- El modelo se descarga solo la primera vez
- Puedes pre-descargar el modelo manualmente antes de iniciar los servicios

### Error 401 en el frontend
- Verificar que el usuario tiene un token v√°lido
- Comprobar que SECRET_KEY coincide con auth-service

### Base de datos no se conecta
- Verificar que chatbot-db est√° saludable: `docker ps`
- Revisar credenciales en variables de entorno
- Comprobar conectividad de red

## Desarrollo y Extensiones

### Agregar Nuevos T√≥picos a la Knowledge Base

Editar `knowledge_base.py` y agregar informaci√≥n en la estructura `KNOWLEDGE_BASE`:

```python
KNOWLEDGE_BASE = {
    "nuevo_topico": {
        "descripcion": "...",
        "detalles": [...]
    }
}
```

### Personalizar Respuestas del Bot

Modificar el contexto del sistema en `get_knowledge_context()`:

```python
def get_knowledge_context() -> str:
    context = """
    Eres un asistente... [personalizar tono y comportamiento]
    """
    return context
```

### Agregar Nuevos Endpoints

En `main.py`, agregar nuevas rutas siguiendo el patr√≥n:

```python
@app.post("/custom-endpoint")
async def custom_endpoint(
    data: CustomModel,
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_session)
):
    # L√≥gica personalizada
    pass
```

## Seguridad

- ‚úÖ Autenticaci√≥n JWT requerida para todos los endpoints
- ‚úÖ Validaci√≥n de permisos por usuario
- ‚úÖ Rate limiting en OpenAI (manejo de errores 429)
- ‚úÖ Sanitizaci√≥n de inputs
- ‚úÖ Logs de auditor√≠a de conversaciones
- ‚úÖ Datos sensibles no se env√≠an a OpenAI

## Contribuir

Para contribuir al desarrollo del chatbot:

1. Crear rama feature: `git checkout -b feature/nueva-funcionalidad`
2. Implementar cambios y tests
3. Documentar en este README
4. Crear Pull Request

## Licencia

Proyecto acad√©mico - Universidad [Nombre] - 2025

---

**Desarrollado con ‚ù§Ô∏è para mejorar la experiencia del usuario**

